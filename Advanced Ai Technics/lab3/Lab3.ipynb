{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3945d394-617d-4cc1-954c-71f721fd0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37291f04-b2e7-4047-b094-9d41aae4cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prompt_format = \"\"\"[INST]{}[/INST]\\n{}\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def format_prompt(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        text = prompt_format.format(question, answer) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }\n",
    "    \n",
    "max_length = 128\n",
    "\n",
    "def filter_examples(row):\n",
    "    # TODO: Finish this function to filter out too long examples\n",
    "    # based on tokenized text length.\n",
    "    tokenized_text_length = len(row)\n",
    "    return tokenized_text_length < max_length\n",
    "\n",
    "    \n",
    "# Loading the main subset of the dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "dataset = dataset.map(format_prompt, batched=True)\n",
    "dataset = dataset.filter(filter_examples)\n",
    "\n",
    "\n",
    "# TODO: Complete the dataset split by selecting appropriate subsets\n",
    "# and selecting 8 examples for the test set,\n",
    "# and 2048 examples for the train set\n",
    "test_dataset = dataset['test'].select(range(8))\n",
    "train_dataset = dataset['train'].select(range(2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144405a8-2a50-4d35-a7c4-d1d4e8658193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/student/miniconda3/envs/s207250/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer:\n",
      "<|begin_of_text|>[INST]Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?[/INST] [INST]The farmer is in the market every day and sells 15 ducks a day at a\n",
      "Ground truth:\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "Model answer:\n",
      "<|begin_of_text|>[INST]A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?[/INST]<|end_of_text|>\n",
      "Ground truth:\n",
      "It takes 2/2=<<2/2=1>>1 bolt of white fiber\n",
      "So the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\n",
      "#### 3\n"
     ]
    }
   ],
   "source": [
    "#task 2\n",
    "def compare(index):\n",
    "    question = (\"[INST]{}[/INST]\".format(test_dataset[index]['question']))\n",
    "    encoded = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        encoded.to(\"cuda\"),\n",
    "        #max_length = 10000,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "    print(\"Model answer:\")\n",
    "    for sentence in outputs[0]:\n",
    "        print(tokenizer.decode(sentence))\n",
    "\n",
    "    print(\"Ground truth:\")\n",
    "    print(test_dataset[index]['answer'])\n",
    "\n",
    "\n",
    "compare(0)\n",
    "compare(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1801fd-4f6d-4004-b185-b64e536153bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraModel, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "loraModel = get_peft_model(model, config)\n",
    "\n",
    "loraModel.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6caac63-4aca-43e2-9bfa-eec9097c1c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49d63b45ae34c1f8ae9dff800bedfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a67365c16f34eea93221a3a4268a53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 01:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.969800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniconda3/envs/s207250/lib/python3.12/site-packages/peft/utils/other.py:1094: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67ef9204-33f04b4b3f9ed55101fdf3a1;c2468302-f7c2-4597-9c7d-be7408c6ef9e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B.\n",
      "  warnings.warn(\n",
      "/home/student/miniconda3/envs/s207250/lib/python3.12/site-packages/peft/utils/save_and_load.py:227: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    num_train_epochs= 1, # TODO: Set number of training epochs\n",
    "    per_device_train_batch_size= 8, # TODO: Set batch size\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5, # TODO: Set learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.2,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataset_text_field='text', # TODO: Set dataset text field\n",
    "    max_seq_length=128, # TODO: Set maximum sequence length\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    seed=42,\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1024,\n",
    "    output_dir=\"tmp\",\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    loraModel,\n",
    "    train_dataset = train_dataset,\n",
    "    args = sft_config\n",
    ")\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c451b5c-b580-43b3-a86a-f0214722c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer:\n",
      "<|begin_of_text|>[INST]Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?[/INST]<|end_of_text|>\n",
      "Ground truth:\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "Model answer:\n",
      "<|begin_of_text|>[INST]A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?[/INST]<|end_of_text|>\n",
      "Ground truth:\n",
      "It takes 2/2=<<2/2=1>>1 bolt of white fiber\n",
      "So the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\n",
      "#### 3\n"
     ]
    }
   ],
   "source": [
    "#task 5\n",
    "def compareTrained(index):\n",
    "    question = (\"[INST]{}[/INST]\".format(test_dataset[index]['question']))\n",
    "    encoded = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = loraModel.generate(\n",
    "        encoded.to(\"cuda\"),\n",
    "        #max_length = 10000,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "    print(\"Model answer:\")\n",
    "    for sentence in outputs[0]:\n",
    "        print(tokenizer.decode(sentence))\n",
    "\n",
    "    print(\"Ground truth:\")\n",
    "    print(test_dataset[index]['answer'])\n",
    "\n",
    "\n",
    "compareTrained(0)\n",
    "compareTrained(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51ab2b-0f7f-49ab-afbd-f9d969ad41d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
